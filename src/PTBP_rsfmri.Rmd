---
title: 'The Pediatric Template of Brain Perfusion: Resting state functional  mri processing'
author: "Brian B. Avants et al."
date: "`r Sys.Date()`"
output:
  beamer_presentation:
    colortheme: dolphin
    fonttheme: structurebold
    highlight: tango
    incremental: yes
    theme: AnnArbor
    includes:
      in_header: mystyle.tex
    toc: yes
  ioslides_presentation:
    highlight: tango
    incremental: yes
---

```{r setup,eval=TRUE,results='hide',warning=FALSE,echo=FALSE}
library(ANTsR)
# set this for your own compilation
dd=path.expand( "~/data/antsExamples/ANTsTutorial/data/" )
pre=paste(dd,"101_",sep='')
post=paste('/tmp/test_')
fmrifn=paste(pre,"pcasl.nii.gz",sep='')
if ( ! file.exists(fmrifn) ) stop(paste("No",fmri))
```

# Overview and resources

## Overview

This is a compilable document with source code located here:

[https://github.com/stnava/ANTsTutorial](https://github.com/stnava/ANTsTutorial)

To get this source, do:

```bash
git clone http://github.com/stnava/ANTsTutorial.git
```
It is expected that you will compile and, after downloading data,
run this:

```r
rmarkdown::render("src/PTBP_rsfmri.Rmd")
```

from within the cloned `ANTsTutorial` directory.  The document needs the [complete PTBP subject data](https://github.com/jeffduda/NeuroBattery)
discussed below. It depends on *R*, *rmarkdown* and *ANTsR* primarily.

Herein, [links are in this color](http://lmgtfy.com).

## Overview

The Pediatric Template of Brain Perfusion (PTBP) [at figshare](http://figshare.com/articles/The_Pediatric_Template_of_Brain_Perfusion_PTBP_/923555).

- Free multiple modality MRI data with demographics and psychometrics
- Collected to aid as a reference for normal variability in structure and function during adolescence
- The data is accompanied by an [organized csv file](http://files.figshare.com/1699436/ptbp_summary_demographics.csv)
- The full data is available at [figshare](http://figshare.com/articles/The_Pediatric_Template_of_Brain_Perfusion_PTBP_/923555)
- Here we use a single subject from this dataset.
- There is also a template contained in the download.

## Download the neurobattery data

From within the ANTsTutorial directory:

```bash
git clone http://github.com/jeffduda/NeuroBattery.git
```

This will give you both raw and processed output
for a single multiple modality subject.

**FIXME**: need to actually run to get output ... that would take too long.

We test (occasionally) against this reference output to monitor
stability of our processing.

## Resting state fMRI

We present *basic* processing strategies here:

* Motion correction

* Mapping to subject-space T1

* Mapping to a T1 group template

* Data-driven nuisance modeling

* Network metrics and visualization

* **many of these strategies are reused for DWI and ASL**


# Step by step preprocessing

## Motion correction

We do more or less the same thing for any
time series modality.

```{r motion,eval=TRUE}
fmri = antsImageRead( fmrifn )
amc = antsMotionCalculation(fmri,moreaccurate=0)
```

* Will motion correct with affine map

* Will produce a mask and motion parameters

* "moco_img"     "moco_params"  "moco_avg_img" "moco_mask"  "dvars"


## Visualize motion parameters: Matrix

```{r vizmocoaff}
plot( ts( amc$moco_params[,3:11] ) )
```

## Visualize motion parameters: Translation

```{r vizmoco2}
plot( ts( amc$moco_params[,12:ncol(amc$moco_params)] ) )
```

## Visualize nuisance parameters: DVARS

```{r vizmoco3}
plot( ts( amc$dvars ) )
```

## Look at the calculated average

```{r mocoimg,echo=FALSE}
plot( amc$moco_avg_img, axis=3 )
```

## Look at the calculated mask (gradient image)

```{r mocomask,echo=FALSE}
gradmask=amc$moco_mask %>% iMath("Grad") %>% iMath("Normalize")
plot( amc$moco_avg_img, gradmask , axis=3,
  window.overlay=c(0.5,1) )
```

## Mapping to subject-space T1

We now have an "anatomical" image ... the average BOLD.

Let's quickly map to T1.

```{r distcor,eval=TRUE}
t1seg=paste(pre,"seg.nii.gz",sep='')
t1n4=paste(pre,"t1.nii.gz",sep='')
if ( file.exists(t1seg)  )
  {
  t1seg=antsImageRead( t1seg )
  t1n4=antsImageRead( t1n4 )
  t1brain=t1n4 * thresholdImage( t1seg, 1, 6 )
  # might modify above depending on coverage
  }
bavgn3=n3BiasFieldCorrection( amc$moco_avg_img, 2 ) * amc$moco_mask
# disco=antsRegistration( bavgn3, t1brain, "SyNBold" ) # probably performs better
disco=antsRegistration( bavgn3, t1brain, "SyN" )
segw=antsApplyTransforms( bavgn3, t1seg,
  disco$fwdtransforms, interpolator = "NearestNeighbor")
```

## Mapped T1

```{r mappedt1}
plot( disco$warpedmovout, axis=3 )
```


## Target image

```{r bavgn3}
plot(  bavgn3 , axis=3 )
```

## Mapped Segmentation

```{r mappedseg}
plot( bavgn3, segw, window.overlay=c(0,5), axis=3 )
```

## Mapping to a T1 group template

We concatenate the distortion correction parameters
with the group template mapping.

Then apply to the labels to bring them to the BOLD
subject space.

Exercise?

We already did this so let's just read the labels.

```{r aal}
aalfn=paste(pre,"aal.nii.gz",sep='')
if ( file.exists(aalfn) ) {
  aalimg = antsImageRead( aalfn )
} 
```

## A mapping exercise: Template to T1 to Bold

```{r mapaal}
if ( ! file.exists(aalfn) ) {
  mni = antsImageRead( getANTsRData( "mni" ) )   # download template data
  mnia = antsImageRead( getANTsRData( "mnia" ) ) # download template labels
  areg = antsRegistration( disco$warpedmovout, mni, typeofTransform = 'SyN' )
  aalimg = antsApplyTransforms( disco$warpedmovout, mnia, transformlist = areg$fwdtransforms,
                              interpolator = 'nearestneighbor' )
  plot( bavgn3, aalimg, window.overlay=c(0,max(aalw)), axis=3 )
  }
```

How can we improve on this approach?  

Can we exploit transform composition?   Use a better reference?

## A mapping solution: Template to T1 to Bold

```{r mapaal2,eval=FALSE}
mni = antsImageRead( getANTsRData( "mni" ) )   # download template data
mnia = antsImageRead( getANTsRData( "mnia" ) ) # download template labels
areg2 = antsRegistration( t1brain, mni, typeofTransform = 'SyN' )
concatMap = c( disco$fwdtransforms, areg2$fwdtransforms )
aalimg = antsApplyTransforms( disco$warpedmovout, mnia, 
    transformlist = concatMap,
    interpolator = 'nearestneighbor' )
```

Why might this be better?

## View the labels

```{r vizaal,echo=FALSE}
plot( bavgn3 , aalimg , axis=3 )
```

## Data-driven nuisance modeling

Nick prepackaged a generic processor for this ...

* We have a few methods but `compcor` is nice.

```{r nickpro}
boldpre=preprocessfMRI( fmri,
  numberOfCompCorComponents = 6,
  doMotionCorrection = 0,
  useMotionCorrectedImage = 0,
  spatialSmoothingType='none',
  spatialSmoothingParameters = mean( antsGetSpacing(fmri)[1:3] ),
  residualizeMatrix = TRUE,
  frequencyLowThreshold=0.01,
  frequencyHighThreshold=0.1
  )
```

## Preprocessor outputs

Nick prepackaged a generic processor for this ...

* This redoes a few things we did above but now
you know a little about what's happening inside.

* Should we smooth?

```bash
> names(boldpre)
[1] "cleanBoldImage"    "maskImage"         "DVARS"
[4] "DVARSpostCleaning" "FD"                "globalSignal"
[7] "nuisanceVariables"
```

## Look at FD and DVARS

```{r fd,echo=FALSE}
plot( ts( cbind( boldpre$FD, boldpre$DVARS ) ) )
```


## Look at FD and DVARS: Post clean

```{r fdpost,echo=FALSE}
plot( ts( cbind( boldpre$FD, boldpre$DVARSpostCleaning ) ) )
```



## Global signal

```{r glob,echo=FALSE}
tsmatpre = timeseries2matrix( fmri,
  boldpre$maskImage )
tsmat = timeseries2matrix( boldpre$cleanBoldImage,
  boldpre$maskImage )
tsmatff = frequencyFilterfMRI( tsmat, tr = 4, freqLo = 0.01, freqHi = 0.1  )
plot( ts( cbind( rowMeans(tsmatpre) ,
  rowMeans(tsmatff) ) ) )
```

## Nuisance modeling: a little detail ...

Nuisance variables can take several different forms.

* frequency filtering removes "non-neural signal" (putatively)

* tissue-specific nuisance variables try to capture non-neural signal in non-neural tissue

* data-driven methods, such as `compcor` or ICA, seek to estimate the nuisance signal from the data

* at this time, i prefer `compcor` .... why might we prefer it?


## Tissue nuisance variables

Get tissue signals.

```{r tissueEx}
csfmat = timeseries2matrix( amc$moco_img, 
      thresholdImage( segw, 1, 1 ) )
wmmat = timeseries2matrix( amc$moco_img, 
      thresholdImage( segw, 3, 3 ) )
plot( ts( cbind( rowMeans( csfmat ),
                 rowMeans( wmmat )  ) ) )
```

## CompCor

* Find high variance voxels ( tend to be in CSF and WM )

* Perform PCA on these voxels

* Use the top $k$ components as covariates of no interest

* Advantages: automated, fast, principled, validated with physiological measurements

```{r compcorEx}
# ccmat = timeseries2matrix( amc$moco_img, amc$moco_mask )
mycompcor = compcor( tsmatff, 10 )
print( colnames( mycompcor ) )
```



## CompCor: Plot

```{r compcorEx2}
plot( ts( mycompcor ) )
```


# Build the network


## Now we can construct time-series averages for each region

Just use matrix multiplication.

```{r tsavg,eval=TRUE}
data("aal")
labmat = labels2matrix( aalimg,  boldpre$maskImage, 
                        targetLabels = aal$label_num )
residmat = residuals( lm( tsmatff ~ mycompcor  ) )
tsavg = residmat %*% t(labmat)
tsavgcor = antsrimpute( cor(tsavg) )
rownames( tsavgcor ) = aal$label_name
colnames( tsavgcor ) = aal$label_name
```

## Look quickly at the correlations

```{r ph,echo=FALSE}
pheatmap::pheatmap( tsavgcor )
```

## Network metrics

Now we can estimate connectivity from the BOLD data.

We'll use some nice *ANTsR* tricks for this.

```{r getgraphs,eval=TRUE}
gmet <- makeGraph( tsavgcor, graphdensity = 0.1, 
                   communityMethod = 'greedy' )
```

Outputs

```r
> names(gmet)
 [1] "mygraph"           "centrality"        "closeness"
 [4] "pagerank"          "degree"            "betweeness"
 [7] "localtransitivity" "strength"          "degcent"
[10] "effinv"            "community"         "walktrapcomm"
[13] "adjacencyMatrix"  
```

# Visualize the network


## Network visualization with `igraph`

```{r igrviz}
plot( gmet$mygraph )
```

## Community visualization with `igraph`

```{r comviz}
plot( gmet$community, gmet$mygraph)
```


## Look at the connection matrix

```{r connmat}
 metweights=gmet$adjacencyMatrix[1:90,1:90]
 image(metweights)
```

## Network visualization in brain space

This is something we have to run "by hand"

```{r igrvizbrain,eval=FALSE}
 cnt<-getCentroids( aalimg, clustparam = 0 )
 aalcnt<-cnt[1:90,1:3] # cortex
 brain<-renderSurfaceFunction( surfimg=
   list( boldpre$maskImage ) , alphasurf=0.1,
   smoothsval = 1.5 )
 metweights[ metweights < 0.01 ] = 0
 plotBasicNetwork( centroids = aalcnt, brain, weights=metweights )
```

## Discussion

* There is also `antsBOLDNetworkAnalysis` but it makes many assumptions
that may not hold.  Need to look at the code.

* Maybe we should have thrown a few frames away ... how? hint: `matrix2timeseries` ...

* Maybe we should have imputed some data ... see `antsBOLDNetworkAnalysis`

* Maybe we should have used the tissue segmentation ....

* How might we do group statistics?

* We mostly produce node metrics but edge metrics are good too ...

* Any other thoughts?
